## How to Use `main.py` (Command Line Interface)

The `main.py` script allows you to run the LLM evaluation framework from your command line. This is useful for batch processing, automated testing, or integrating the evaluation into other scripts.

### Prerequisites

* Ensure you have Python installed.
* Install the required dependencies from `requirements.txt`:
    ```bash
    pip install -r requirements.txt
    ```
* Navigate to the root directory of the project (`GenAI_Evaluation_Tool-main`) in your terminal.

### Core Functionalities

1.  **Generate Mock Data:**
    You can generate sample evaluation data in the flat JSON and CSV formats. This is helpful for understanding the expected input structure or for quick tests.

    * **Command:**
        ```bash
        python main.py --generate-mock-data
        ```
    * **Output:** This will create `llm_eval_mock_data_no_contexts.json` and `llm_eval_mock_data_no_contexts.csv` inside the `data/` directory by default (note: `_no_contexts` was added to the filename in the updated `mock_data_generator.py` example).
    * **Custom Output:** To specify a different base path and filename for the generated mock data:
        ```bash
        python main.py --generate-mock-data --mock-data-output-base path/to/your/custom_mock_data
        ```
        This will create `custom_mock_data.json` and `custom_mock_data.csv` in the specified path.

2.  **Run Evaluation from an Input File:**
    You can evaluate LLM responses using your own data provided in a JSON, Excel (`.xlsx`), or CSV (`.csv`) file. The file must follow the "flat format" where each row represents a single evaluation instance.

    * **Required Columns in Input File:**
        * `task_type`: The type of task (e.g., `rag_faq`, `summarization`).
        * `model`: An identifier for the LLM or model configuration being evaluated.
        * `question`: The input query or text provided to the LLM.
        * `ground_truth`: The ideal or reference answer/label.
        * `answer`: The actual response generated by the LLM.
    * **Optional Columns:**
        * `id`: A unique identifier for the test case (recommended).
        * `test_description`: A brief description of the test case.
        * `ref_facts`: Comma-separated factual statements relevant for the `fact_presence_score` metric.
        * `ref_key_points`: Comma-separated key points relevant for the `completeness_score` metric.
        * *(Note: The `contexts` field has been removed from the default processing.)*

    * **Command:**
        ```bash
        python main.py --input-file path/to/your/input_data.csv
        ```
        Replace `path/to/your/input_data.csv` with the actual path to your data file. It can also be `.xlsx` or `.json`.

    * **Output Directory:** By default, reports are saved in a directory named `reports/` in the project root.
        To specify a different output directory:
        ```bash
        python main.py --input-file path/to/your/input_data.json --output-dir custom_reports_folder
        ```

    * **Generated Reports:**
        Upon successful evaluation, the following files will be created in the output directory:
        * `evaluation_report_individual_scores_<timestamp>.csv`: Contains detailed scores for every individual test case, including all input fields and calculated metric scores.
        * `evaluation_report_aggregated_summary_<timestamp>.csv`: Provides scores aggregated by `task_type` and `model`, showing the average for each numeric metric and the number of samples.
        * `evaluation_report_aggregated_summary_<timestamp>.md`: A Markdown version of the aggregated summary, suitable for quick viewing or inclusion in documentation.

### Example Workflow

1.  **Prepare your data:** Create a CSV file (e.g., `my_eval_data.csv`) with columns like `id, task_type, model, question, ground_truth, answer, ref_facts`.
2.  **Run evaluation:**
    ```bash
    python main.py --input-file my_eval_data.csv
    ```
3.  **Check results:** Look in the `reports/` directory for the generated CSV and Markdown files.

### Comparison with Streamlit UI

* `main.py` is designed for **batch processing** and generating static reports.
* It **does not** offer the interactive data editing, dynamic visualizations (charts, detailed drill-downs), or the guided metric tutorials found in the Streamlit web application.
* The core evaluation logic (`evaluate_model_responses` function) is shared between `main.py` and the Streamlit app, ensuring consistency in how metrics are calculated.


