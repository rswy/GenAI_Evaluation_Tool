# src/ui_components/data_management_view.py
import streamlit as st
import pandas as pd
import numpy as np
import datetime

# Assuming these are imported from app_config in the main app and passed if needed,
# or imported directly here if app_config is in the same path.
from app_config import REQUIRED_FIELDS_ADD_ROW, OPTIONAL_FIELDS_ADD_ROW_INFO
from tasks.task_registry import get_supported_tasks # Assuming task_registry is accessible

def render_data_editor_tab():
    """Renders the 'View/Edit/Add Data' tab."""
    st.header("Manage Evaluation Data")
    st.markdown("Manually add new evaluation rows or edit existing data. Data loaded/generated via sidebar appears here.")
    
    # --- Add New Evaluation Row Section ---
    st.subheader("Add New Evaluation Row")
    
    def update_input_mode_editor_tab(): # Callback specific to this radio's key
        st.session_state.add_row_input_mode = st.session_state.add_row_input_mode_selector_data_tab
    
    st.radio(
        "Input Mode for Adding Rows:",
        ("Easy (Required Fields Only)", "Custom (Select Additional Fields)"),
        key="add_row_input_mode_selector_data_tab", # Unique key
        horizontal=True,
        index=("Easy (Required Fields Only)", "Custom (Select Additional Fields)").index(st.session_state.add_row_input_mode),
        on_change=update_input_mode_editor_tab
    )

    with st.form("add_case_form_editor_tab", clear_on_submit=True):
        col_form_id, col_form_task, col_form_model = st.columns(3)
        with col_form_id: add_id_val = st.text_input("Test Case ID (Optional)", key="add_id_input_editor", placeholder="e.g., rag_case_001")
        with col_form_task: add_task_type_val = st.selectbox("Task Type*", list(get_supported_tasks()), key="add_task_type_select_editor", index=None, placeholder="Select Task...")
        with col_form_model: add_model_val = st.text_input("LLM/Model Config*", key="add_model_input_editor", placeholder="e.g., MyModel_v1.2_temp0.7")
        
        add_question_val = st.text_area("Question / Input Text*", key="add_question_input_editor", placeholder="Input query, text to summarize/classify, or chatbot utterance.", height=100)
        add_ground_truth_val = st.text_area("Ground Truth / Reference*", key="add_ground_truth_input_editor", placeholder="Ideal answer, reference summary, correct label, or reference response.", height=100)
        add_answer_val = st.text_area("LLM's Actual Answer / Prediction*", key="add_answer_input_editor", placeholder="Actual output generated by the LLM.", height=100)

        form_add_test_description = None; form_add_ref_facts = None; form_add_ref_key_points = None; form_add_contexts = None

        if st.session_state.add_row_input_mode == "Custom (Select Additional Fields)":
            st.markdown("---"); st.markdown("**Custom Fields (Optional):**")
            form_add_test_description = st.text_input(OPTIONAL_FIELDS_ADD_ROW_INFO["test_description"]["label"], key="add_description_input_custom_editor", placeholder=OPTIONAL_FIELDS_ADD_ROW_INFO["test_description"]["placeholder"])
            st.caption(OPTIONAL_FIELDS_ADD_ROW_INFO["test_description"]["metric_info"])
            form_add_ref_facts = st.text_input(OPTIONAL_FIELDS_ADD_ROW_INFO['ref_facts']['label'], key="add_ref_facts_input_custom_editor", placeholder=OPTIONAL_FIELDS_ADD_ROW_INFO["ref_facts"]["placeholder"])
            st.caption(OPTIONAL_FIELDS_ADD_ROW_INFO['ref_facts']['metric_info'])
            form_add_ref_key_points = st.text_input(OPTIONAL_FIELDS_ADD_ROW_INFO['ref_key_points']['label'], key="add_ref_key_points_input_custom_editor", placeholder=OPTIONAL_FIELDS_ADD_ROW_INFO["ref_key_points"]["placeholder"])
            st.caption(OPTIONAL_FIELDS_ADD_ROW_INFO['ref_key_points']['metric_info'])
            form_add_contexts = st.text_area("Contexts (Optional, for RAG)", key="add_contexts_input_custom_editor", placeholder="Provide context snippets if relevant for RAG tasks.", height=75)
        else: 
            form_add_test_description = st.text_input(OPTIONAL_FIELDS_ADD_ROW_INFO["test_description"]["label"], key="add_description_input_easy_editor", placeholder=OPTIONAL_FIELDS_ADD_ROW_INFO["test_description"]["placeholder"])
            st.caption(OPTIONAL_FIELDS_ADD_ROW_INFO["test_description"]["metric_info"])

        submitted_add_row = st.form_submit_button("➕ Add Evaluation Row to Editor")
        
        if submitted_add_row:
            missing_fields = []
            if not add_task_type_val: missing_fields.append("Task Type")
            if not add_model_val.strip(): missing_fields.append("LLM/Model Config")
            if not add_question_val.strip(): missing_fields.append("Question / Input Text")
            if not add_ground_truth_val.strip(): missing_fields.append("Ground Truth / Reference")
            if not add_answer_val.strip(): missing_fields.append("LLM's Actual Answer")

            if missing_fields:
                st.error(f"Required fields (*) missing: {', '.join(missing_fields)}.")
            else:
                current_df = st.session_state.edited_test_cases_df
                final_add_id = add_id_val.strip() if add_id_val and add_id_val.strip() else f"manual_{pd.Timestamp.now().strftime('%Y%m%d%H%M%S%f')}"
                if not current_df.empty and 'id' in current_df.columns and final_add_id in current_df['id'].astype(str).values:
                    st.error(f"ID '{final_add_id}' already exists. Please use a unique ID or leave blank for auto-generation.")
                else:
                    new_row_dict = {
                        'id': final_add_id, 'task_type': add_task_type_val, 
                        'model': add_model_val.strip(), 'question': add_question_val.strip(),
                        'ground_truth': add_ground_truth_val.strip(), 'answer': add_answer_val.strip(),
                        'test_description': form_add_test_description.strip() if form_add_test_description else None,
                        'ref_facts': form_add_ref_facts.strip() if form_add_ref_facts else None,
                        'ref_key_points': form_add_ref_key_points.strip() if form_add_ref_key_points else None,
                        'contexts': form_add_contexts.strip() if form_add_contexts else None,
                    }
                    new_row_dict_cleaned = {k: v for k, v in new_row_dict.items() if v is not None}
                    new_row_df = pd.DataFrame([new_row_dict_cleaned])
                    
                    if st.session_state.edited_test_cases_df.empty: 
                        st.session_state.edited_test_cases_df = new_row_df.fillna('')
                    else:
                        for col in new_row_df.columns:
                            if col not in st.session_state.edited_test_cases_df.columns:
                                st.session_state.edited_test_cases_df[col] = np.nan 
                        st.session_state.edited_test_cases_df = pd.concat(
                            [st.session_state.edited_test_cases_df, new_row_df], ignore_index=True
                        ).fillna('') 
                    st.success(f"Row '{new_row_dict['id']}' added to editor.")
    
    # --- Data Editor Section ---
    st.divider(); st.subheader("Data Editor")
    if isinstance(st.session_state.edited_test_cases_df, pd.DataFrame) and not st.session_state.edited_test_cases_df.empty:
        st.markdown("Edit data below. Changes are used when 'Run Evaluation' is clicked. Add/delete rows as needed.")
        editor_col_order = ['id', 'task_type', 'model', 'test_description', 
                            'question', 'ground_truth', 'answer', 
                            'ref_facts', 'ref_key_points', 'contexts'] 
        current_cols_in_df = st.session_state.edited_test_cases_df.columns.tolist()
        for col in editor_col_order:
            if col not in current_cols_in_df: st.session_state.edited_test_cases_df[col] = '' 
        final_editor_cols = [col for col in editor_col_order if col in st.session_state.edited_test_cases_df.columns]
        remaining_cols_for_editor = sorted([col for col in st.session_state.edited_test_cases_df.columns if col not in final_editor_cols])
        final_editor_cols.extend(remaining_cols_for_editor)
        df_for_editor_display = st.session_state.edited_test_cases_df[final_editor_cols].copy().fillna('')

        edited_df_from_editor = st.data_editor(
            df_for_editor_display, num_rows="dynamic", use_container_width=True, key="data_editor_main_view"
        )
        st.session_state.edited_test_cases_df = edited_df_from_editor.copy() 
        
        if not st.session_state.edited_test_cases_df.empty:
             csv_edited_data = st.session_state.edited_test_cases_df.fillna('').to_csv(index=False).encode('utf-8')
             st.download_button("⬇️ Download Edited Data (CSV)", csv_edited_data, f"edited_eval_cases_{datetime.datetime.now():%Y%m%d_%H%M%S}.csv", "text/csv", key="dl_edited_data_csv_view")
    else: st.info("No data loaded. Use sidebar to load/generate or add rows using the form.")


def render_data_format_guide_tab():
    """Renders the 'Data Format Guide' tab."""
    st.header("Input Data Format Guide (Flat Format)")
    st.markdown("Framework expects input (JSON, CSV, Excel) in a **flat format**. Each row is one evaluation instance.")
    st.markdown("**Required Columns:** `task_type`, `model`, `question`, `ground_truth`, `answer`.")
    st.markdown("**Optional Columns for Specific Metrics:**")
    st.markdown(f"- `ref_facts`: {OPTIONAL_FIELDS_ADD_ROW_INFO['ref_facts']['label']}. {OPTIONAL_FIELDS_ADD_ROW_INFO['ref_facts']['metric_info']}")
    st.markdown(f"- `ref_key_points`: {OPTIONAL_FIELDS_ADD_ROW_INFO['ref_key_points']['label']}. {OPTIONAL_FIELDS_ADD_ROW_INFO['ref_key_points']['metric_info']}")
    st.markdown("**Other Optional Columns:**")
    st.markdown("- `id`: Unique row identifier (recommended).")
    st.markdown("- `test_description`: Brief description of the test case.")
    st.markdown("- `contexts`: (Optional for RAG) Context snippets for RAG tasks.")
    st.subheader("Example Rows (Conceptual):")
    example_data_guide = [
        {'id': 'rag_001', 'task_type': 'rag_faq', 'model': 'ModelAlpha', 'test_description': 'Capital of France', 'question': 'Info on Paris?', 'contexts': 'Paris is capital...', 'ground_truth': 'Paris is the capital of France and a major global city.', 'answer': 'Paris is France\'s capital.', 'ref_facts': 'Paris is capital of France', 'ref_key_points': 'Capital city,Global city status'},
        {'id': 'sum_001', 'task_type': 'summarization', 'model': 'ModelBeta', 'test_description': 'AI Summary', 'question': 'Summarize AI impact.', 'contexts': '', 'ground_truth': 'AI has transformed industries...', 'answer': 'AI changed many fields.', 'ref_facts': '', 'ref_key_points': 'Industry transformation,Societal impact'}
    ]
    st.dataframe(pd.DataFrame(example_data_guide).fillna(''))
